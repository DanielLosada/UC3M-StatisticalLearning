---
title: "Assigment2.Rmd"
output: html_document
date: "2025-03-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data generation

```{r Data simulation}
library(ggplot2)
library(mvtnorm)
library(scatterplot3d)

Mu1 = c(1,1)
Mu2 = c(7,7)
Sigma1 = matrix(c(2, 1, 1, 1), 2,2)
Sigma2 = matrix(c(2, 2, 2, 5), 2,2)

pi = 1/3
n = 300

set.seed(2)
data = matrix(0, n, 2)
z = rep(0,n)
for (i in 1:n){
  z[i] = rbinom(1,1,pi)
  if (z[i] ==1){
    data[i,] = rmvnorm(1, Mu1,Sigma1)
  }else{
    data[i,] = rmvnorm(1, Mu2,Sigma2)
  }
}

to.plot = data.frame(x = data[,1], 
                  y = data[,2], 
                  class =  z)
ggplot(to.plot)+ aes(x, y, color = class)+
  geom_point()+geom_density_2d()
```
# Functions
```{r Functions for EM Components (Basic Two-Cluster Version)}
#compute_gamma <- function(pi, phi1, phi2){
#  (pi*phi1)/((pi*phi1)+((1-pi)*phi2))
#}

#compute_l <- function(pi, gamma, phi1, phi2){
#  sum(gamma*log(pi) + (1-gamma)*log(1-pi)) + sum(gamma*log(phi1) + (1-gamma)*log(phi2))
#}

initialize_values <- function(data, n, d, K) {
  set.seed(123)  # Ensure reproducibility
  
  # Initialize mixing proportions π (equal for all Gaussians)
  Pis <- rep(1/K, K)
  
  # Initialize means µ randomly chosen from the dataset
  Mus <- data[sample(1:n, K), , drop=FALSE]  # Select K random points
  
  # Initialize covariance matrices Σ with the overall covariance of the data
  Sigmas <- lapply(1:K, function(i) cov(data))  # Copy initial covariance for all clusters
  
  return(list(Pis = Pis, Mus = Mus, Sigmas = Sigmas))
}

convert_to_rgb <- function(k, segmented_img){
  my_colors <- rainbow(K)
  
  # Suppose segmented_img is the matrix of labels
  height <- nrow(segmented_img)
  width  <- ncol(segmented_img)
  
  # Create an empty 3D array for the colored image
  segmented_rgb <- array(0, dim = c(height, width, 3))
  
  # Fill in the array by mapping each cluster to its color
  for (k in seq_len(K)) {
    # Convert the k-th color to an (R,G,B) vector in [0,1]
    rgb_vals <- col2rgb(my_colors[k]) / 255
    
    # Where the segmentation == k, assign the color
    mask <- (segmented_img == k)
    segmented_rgb[,,1][mask] <- rgb_vals[1]  # R channel
    segmented_rgb[,,2][mask] <- rgb_vals[2]  # G channel
    segmented_rgb[,,3][mask] <- rgb_vals[3]  # B channel
  }
  return(segmented_rgb)
}
```


# EM Algorithm

```{r}

#initial values
tol = 10^-3
pi = 0.5
mu1 = data[1,]
mu2 = data[150,]
sigma1 = cov(data)
sigma2 = cov(data)


phi1 = dmvnorm(data, mu1, sigma1) # φ1 en el pdf
phi2 = dmvnorm(data, mu2, sigma2) # φ1 en el pdf

L_temp = compute_l(pi, compute_gamma(pi, phi1, phi2), phi1, phi2)
cat("Initial log-likelihood: ", L_temp, "\n")

L <- 0
iteration_counter <- 0

L = L_temp
# Expectation step
gamma <- compute_gamma(pi, phi1, phi2)

# Maximization step
# mu1
mu1 <- apply(gamma*data/sum(gamma), 2, sum)
#mu2
mu2 <- apply((1-gamma)*data/sum(1-gamma), 2, sum)
# sigma1
sigma1 <- cov.wt(data, wt = gamma, method = "ML")$cov
# sigma2
sigma2 <- cov.wt(data, wt = 1-gamma, method = "ML")$cov
#pi
pi <- mean(gamma)


phi1 = dmvnorm(data, mu1, sigma1)
phi2 = dmvnorm(data, mu2, sigma2)
L_temp = compute_l(pi, gamma, phi1, phi2)
iteration_counter <- iteration_counter + 1
cat("Iteration: ", iteration_counter, " | Current log-likelihood: ", L_temp, "\n")

  

```


```{r}
compute_gamma <- function(pi, phi1, phi2){
  (pi*phi1)/((pi*phi1)+((1-pi)*phi2))
}

compute_l <- function(pi, gamma, phi1, phi2){
  sum(gamma*log(pi) + (1-gamma)*log(1-pi)) + sum(gamma*log(phi1) + (1-gamma)*log(phi2))
}

#initial values
tol = 10^-3
pi = 0.5
mu1 = data[1,]
mu2 = data[150,]
sigma1 = cov(data)
sigma2 = cov(data)

EM_algorithm <- function(data, pi, mu1, mu2, sigma1, sigma2, tol = 10^-2){
  
  phi1 = dmvnorm(data, mu1, sigma1) # φ1 en el pdf
  phi2 = dmvnorm(data, mu2, sigma2) # φ1 en el pdf
  
  gamma <- compute_gamma(pi, phi1, phi2)
  L_temp <- compute_l(pi, gamma, phi1, phi2)
  cat("Initial log-likelihood: ", L_temp, "\n")

  L <- 0
  iteration_counter <- 0
  
  while (abs(L-L_temp)>=tol) {
    L = L_temp
    # Expectation step
    gamma <- compute_gamma(pi, phi1, phi2)
    
    # Maximization step
    # mu1
    mu1 <- apply(gamma*data/sum(gamma), 2, sum)
    #mu2
    mu2 <- apply((1-gamma)*data/sum(1-gamma), 2, sum)
    # sigma1
    sigma1 <- cov.wt(data, wt = gamma, method = "ML")$cov
    # sigma2
    sigma2 <- cov.wt(data, wt = 1-gamma, method = "ML")$cov
    #pi
    pi <- mean(gamma)
    
    
    phi1 = dmvnorm(data, mu1, sigma1)
    phi2 = dmvnorm(data, mu2, sigma2)
    L_temp = compute_l(pi, gamma, phi1, phi2)
    iteration_counter <- iteration_counter + 1
    cat("Iteration: ", iteration_counter, " | Current log-likelihood: ", L_temp, "\n")
  }
  
  return(list(n_iterations=iteration_counter, pi=pi, mu1=mu1, mu2=mu2, sigma1=sigma1, sigma2=sigma2))
}

results <- EM_algorithm(data, pi, mu1, mu2, sigma1, sigma2, tol = tol)

cat("EM converged in ", results$n_iterations, " iterations.\n")

```


```{r Generating Gaussian Synthetic Data}
library(ggplot2)
library(mvtnorm)

generate_gaussian_mixture_data <- function(n, d, K, seed=42) {
  set.seed(seed)
  
  # Equal proportion
  Pis <- rep(1/K, K)  
  
  # Generate random means and covariance matrices for each Gaussian
  Mu <- lapply(1:K, function(i) runif(d, min = 0, max = 10))  # Random means in [0,10]
  Sigma <- lapply(1:K, function(i) {
    A <- matrix(runif(d*d, min=0.5, max=3), d, d)  # Random matrix
    return(t(A) %*% A)  # Ensures positive semi-definiteness
  })
  
  # Initialize data
  data <- matrix(0, n, d)
  z <- sample(1:K, n, replace=TRUE, prob=Pis)  # Assign clusters

  for (i in 1:n) {
    data[i, ] <- rmvnorm(1, Mu[[z[i]]], Sigma[[z[i]]])
  }
  
  return(list(data = data, z = z, Mu = Mu, Sigma = Sigma, Pis = Pis))
}


```

```{r Example usage}
# Example usage
d = 2   # Dimensions: It's the number of values at each observation (data points)
K = 2   # Number of Gaussian components
n = 300 # Number of data points

data_info <- generate_gaussian_mixture_data(n, d, K, seed=2)
data <- data_info$data
Mus <- data_info$Mu
Sigmas <- data_info$Sigma
Pis <- data_info$Pis

z <- data_info$z

if (d == 1) {
  to.plot <- data.frame(x = data[,1], class = as.factor(z))
  ggplot(to.plot) + aes(x, fill = class) + geom_density(alpha = 0.5) +
    labs(title="1D Gaussian Mixture Density", x="X", y="Density") +
    theme_minimal()
}

if (d == 2) {
  to.plot = data.frame(x = data[,1], y = data[,2], class = as.factor(z))
  ggplot(to.plot) + aes(x, y, color = class) + geom_point() + geom_density_2d()
}

if (d == 3) {
  library(scatterplot3d)
  scatterplot3d(data, color = as.numeric(z), pch = 19, 
                main = "3D Gaussian Mixture",
                xlab = "X", ylab = "Y", zlab = "Z")
}
```


```{r}
#tol = 10^-3

#init_values <- initialize_values(data, n, d, K)

#EM_algorithm(data, init_values$Pis[1], init_values$Mus[1,], init_values$Mus[2,], init_values$Sigmas[[1]], init_values$Sigmas[[2]], tol = tol)
```

# Generalization
```{r  Generalized EM Algorithm for K Clusters}
compute_gamma <- function(pi, phi) {
  gamma <- phi * pi
  gamma <- gamma / rowSums(gamma)
  return(gamma)
}

compute_l <- function(pi, gamma, phi) {
  # Generalized log-likelihood for K components
  return(sum(log(rowSums(phi * pi))))
}

EM_algorithm <- function(data, pi, mu, sigma, tol = 10^-2, verbose=FALSE) {
  
  K <- length(pi)  # Number of Gaussian components
  n <- nrow(data)  # Number of data points
  d <- ncol(data)  # Number of dimensions

  # Compute initial phi values for all Gaussians
  phi <- sapply(1:K, function(k) dmvnorm(data, mu[k,], sigma[[k]]))
  
  gamma <- compute_gamma(pi, phi)  # Compute probs of belonging

  L_temp <- compute_l(pi, gamma, phi)  # Compute initial log-likelihood
  if(verbose){
    cat("Initial log-likelihood: ", L_temp, "\n")
  }
  
  L <- 0
  iteration_counter <- 0
  
  while (abs(L - L_temp) >= tol) {
    L <- L_temp

    ## Expectation step ##
    gamma <- compute_gamma(pi, phi)

    ## Maximization step ##
    Nk <- colSums(gamma)  # Number of points assigned to each Gaussian. Sum the columns (prob of belonging of each data point)
    pi <- Nk / n  # Knowing how many are in each Gaussian, we can update the proportion
    
    # Update means
    mu <- t(gamma) %*% data / Nk  # The mean of each component is computed by the sum(prob_belonging_component*observation)/Num_of_points_belonging

    # Update covariance matrices
    sigma <- lapply(1:K, function(k) {
      X_centered <- sweep(data, 2, mu[k,])  # Center data
      sigma_k <- (t(X_centered) %*% (X_centered * gamma[, k])) / Nk[k]
      
      # --- REGULARIZE the covariance to prevent singularities ---
      # For example, add a small value (epsilon) to the diagonal:
      epsilon <- 1e-6
      sigma_k <- sigma_k + diag(epsilon, ncol(data))
      
      return(sigma_k)
    })
    
    # Compute new phi values for all Gaussians
    phi <- sapply(1:K, function(k) dmvnorm(data, mu[k,], sigma[[k]]))
    
    # Compute new log-likelihood
    L_temp <- compute_l(pi, gamma, phi)
    iteration_counter <- iteration_counter + 1
    if(verbose){
      cat("Iteration: ", iteration_counter, " | Current log-likelihood: ", L_temp, "\n")
    }
  }
  
  return(list(n_iterations = iteration_counter, pi = pi, mu = mu, sigma = sigma, gamma = gamma))
}

# Run EM Algorithm
init_values <- initialize_values(data, n, d, K)
results <- EM_algorithm(data, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-3)

cat("EM converged in", results$n_iterations, "iterations.\n")

```


```{r}


# 1) Generate 1D data (K=2, n=300, etc.)
d <- 1
K <- 2
n <- 300
set.seed(687)
data_info_1d <- generate_gaussian_mixture_data(n, d, K)
data_1d <- data_info_1d$data  # shape: (n x 1)
true_labels_1d <- data_info_1d$z

# Quick look at the generated data
df_1d <- data.frame(
  x       = data_1d[,1],
  cluster = factor(true_labels_1d)
)

# Plot the histogram of the generated data (colored by the true cluster)
ggplot(df_1d, aes(x, fill=cluster)) +
  geom_histogram(alpha=0.5, position="identity", bins=30) +
  theme_minimal() +
  ggtitle("Generated 1D Gaussian Mixture")

# 2) Initialize parameters for EM
init_1d <- initialize_values(data_1d, nrow(data_1d), d, K)

# 3) Run EM algorithm with logging
res_1d <- EM_algorithm(
  data   = data_1d,
  pi     = init_1d$Pis,
  mu     = init_1d$Mus,
  sigma  = init_1d$Sigmas,
  tol    = 1e-4
)

cat("1D EM converged in", res_1d$n_iterations, "iterations.\n")

# 5) Assign each data point to the most probable cluster
gamma_1d <- res_1d$gamma
assigned_1d <- apply(gamma_1d, 1, which.max)

# create a confusion matrix to assignate the correct label to each class
cm <- table(true_labels_1d, assigned_1d)
if (cm[1,1] + cm[2,2] < cm[1,2] + cm[2,1]) {
  assigned_1d <- ifelse(assigned_1d == 1, 2, 1)
}
df_1d$assigned <- factor(assigned_1d)

# 6) Visualize final clusters
ggplot(df_1d, aes(x, fill=assigned)) +
  geom_histogram(alpha=0.5, position="identity", bins=30) +
  theme_minimal() +
  ggtitle("1D Gaussian Mixture - EM assigned clusters")
```





```{r}
# 1) Generate 2D data
d <- 2
K <- 2
n <- 300
set.seed(563)
data_info_2d <- generate_gaussian_mixture_data(n, d, K)
data_2d <- data_info_2d$data    # shape: (n x 2)
true_labels_2d <- data_info_2d$z

df_2d <- data.frame(
  x       = data_2d[,1],
  y       = data_2d[,2],
  cluster = factor(true_labels_2d)
)

# Quick plot of the generated data (color = true cluster)
ggplot(df_2d, aes(x, y, color=cluster)) +
  geom_point() +
  geom_density_2d() +
  theme_minimal() +
  ggtitle("Generated 2D Gaussian Mixture")

# 2) Initialize parameters
init_2d <- initialize_values(data_2d, nrow(data_2d), d, K)

# 3) Run EM
res_2d <- EM_algorithm(
  data   = data_2d,
  pi     = init_2d$Pis,
  mu     = init_2d$Mus,
  sigma  = init_2d$Sigmas,
  tol    = 1e-4
)

cat("2D EM converged in", res_2d$n_iterations, "iterations.\n")

# 5) Cluster assignment
gamma_2d <- res_2d$gamma
assigned_2d <- apply(gamma_2d, 1, which.max)

# create a confusion matrix to assignate the correct label to each class
cm <- table(true_labels_2d, assigned_2d)

if (cm[1,1] + cm[2,2] < cm[1,2] + cm[2,1]) {
  assigned_2d <- ifelse(assigned_2d == 1, 2, 1)
}
df_2d$assigned <- factor(assigned_2d)
# 6) Plot the final assigned clusters
ggplot(df_2d, aes(x, y, color=assigned)) +
  geom_point() +
  theme_minimal() +
  ggtitle("2D Gaussian Mixture - EM assigned clusters")

```

```{r}
# Generate 3D data with K = 3 clusters
d <- 3
K <- 3
n <- 300
set.seed(123)
data_info_3d <- generate_gaussian_mixture_data(n, d, K)
data_3d <- data_info_3d$data
true_labels_3d <- data_info_3d$z

# Create a data frame for the initial (true) clustering
df_3d_initial <- data.frame(
  x = data_3d[,1],
  y = data_3d[,2],
  z = data_3d[,3],
  cluster = factor(true_labels_3d)
)

# Plot the true clusters using plotly
library(plotly)
initial_plot <- plot_ly(data = df_3d_initial,
                        x = ~x,
                        y = ~y,
                        z = ~z,
                        type = "scatter3d",
                        mode = "markers",
                        marker = list(size = 4),
                        color = ~cluster,
                        colors = c("red", "green", "blue")) %>%
  layout(scene = list(
           xaxis = list(title = "X"),
           yaxis = list(title = "Y"),
           zaxis = list(title = "Z")
         ),
         showlegend = TRUE)

# Initialize parameters using your existing function
init_3d <- initialize_values(data_3d, nrow(data_3d), d, K)

# Run the EM algorithm (using your existing EM_algorithm function)
res_3d <- EM_algorithm(data_3d, init_3d$Pis, init_3d$Mus, init_3d$Sigmas, tol = 1e-4)

# Assign each data point to the most probable cluster based on EM results
assigned_3d <- apply(res_3d$gamma, 1, which.max)

# Create a data frame for the final (EM-assigned) clusters
df_3d_final <- data.frame(
  x = data_3d[,1],
  y = data_3d[,2],
  z = data_3d[,3],
  cluster = factor(assigned_3d)
)

# Plot the EM-assigned clusters using plotly
final_plot <- plot_ly(data = df_3d_final,
                      x = ~x,
                      y = ~y,
                      z = ~z,
                      type = "scatter3d",
                      mode = "markers",
                      marker = list(size = 4),
                      color = ~cluster,
                      colors = c("red", "green", "blue")) %>%
  layout(scene = list(
           xaxis = list(title = "X"),
           yaxis = list(title = "Y"),
           zaxis = list(title = "Z")
         ),
         showlegend = TRUE)

# Combine the initial and final plots side by side for comparison
combined_plot <- subplot(initial_plot, final_plot, nrows = 1, margin = 0.05)

# Add a global title using an annotation rather than layout(title=...)
combined_plot <- combined_plot %>% layout(
  annotations = list(
    list(
      text = "Comparison of 3D Clusters: True vs. EM Assigned",
      x = 0.5,
      y = 1.05,
      xref = "paper",
      yref = "paper",
      showarrow = FALSE,
      font = list(size = 20)
    )
  )
)

# Display the combined plot
combined_plot

```


```{r}
# Generate 3D data with K = 3 clusters
d <- 3
K <- 3
n <- 300
set.seed(123)
data_info_3d <- generate_gaussian_mixture_data(n, d, K)
data_3d <- data_info_3d$data

# Initialize parameters using your existing function
init_3d <- initialize_values(data_3d, nrow(data_3d), d, K)

# Run the EM algorithm (using your existing EM_algorithm function)
res_3d <- EM_algorithm(data_3d, init_3d$Pis, init_3d$Mus, init_3d$Sigmas, tol = 1e-4)

# Assign each data point to the most probable cluster
assigned_3d <- apply(res_3d$gamma, 1, which.max)

# Create a data frame for plotting
df_3d <- data.frame(
  x = data_3d[,1],
  y = data_3d[,2],
  z = data_3d[,3],
  cluster = factor(assigned_3d)
)

# Use plotly to create a 3D scatter plot with a legend
library(plotly)

plot_ly(data = df_3d,
        x = ~x,
        y = ~y,
        z = ~z,
        type = "scatter3d",
        mode = "markers",
        marker = list(size = 4),
        color = ~cluster,
        colors = c("red", "green", "blue")) %>%
  layout(title = "3D Gaussian Mixture - EM Assigned Clusters",
         scene = list(
           xaxis = list(title = "X"),
           yaxis = list(title = "Y"),
           zaxis = list(title = "Z")
         ))
```

## Fotos

### Lunar
```{r}
library(OpenImageR)

# Load Image
img <- readImage("Melanoma.jpg")
imageShow(img)
```

```{r}
library(OpenImageR)

# Load Image
img <- readImage("Melanoma.jpg")
imageShow(img)

# Convert image into a matrix of pixel values
img_matrix <- apply(img, 3, as.vector)  # Convert each color channel to a vector

# Ensure it's in matrix form (pixels as rows, channels as columns)
img_matrix <- matrix(img_matrix, ncol = 3, byrow = FALSE)
dim(img_matrix)
```

```{r}
K <- 2  # Number of clusters (e.g., skin vs lesion)

# Initialize values
init_values <- initialize_values(img_matrix, nrow(img_matrix), 3, K)

# Run EM Algorithm
results <- EM_algorithm(img_matrix, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-4)

# Assign each pixel to a cluster based on gamma
segmented <- apply(results$gamma, 1, which.max)  # Assign each pixel to the most probable cluster

# Reshape segmented data back into an image format
segmented_img <- matrix(segmented, nrow = dim(img)[1], ncol = dim(img)[2])

# Display segmented image
imageShow(segmented_img)
```
### Stats


```{r}
# Load Image
img <- readImage("Stats.jpg")
imageShow(img)

# Convert image into a matrix of pixel values
img_matrix <- apply(img, 3, as.vector)  # Convert each color channel to a vector

# Ensure it's in matrix form (pixels as rows, channels as columns)
img_matrix <- matrix(img_matrix, ncol = 3, byrow = FALSE)
K <- 3  # Number of clusters (e.g., skin vs lesion)

# Initialize values
init_values <- initialize_values(img_matrix, nrow(img_matrix), 3, K)

# Run EM Algorithm
results <- EM_algorithm(img_matrix, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-2)

# Assign each pixel to a cluster based on gamma
segmented <- apply(results$gamma, 1, which.max)  # Assign each pixel to the most probable cluster

# Reshape segmented data back into an image format
segmented_img <- matrix(segmented, nrow = dim(img)[1], ncol = dim(img)[2])

# Display segmented image
imageShow(segmented_img)
```


### Spaceship
```{r}
# Load Image
img <- readImage("Spaceship.jpg")
imageShow(img)

# Convert image into a matrix of pixel values
img_matrix <- apply(img, 3, as.vector)  # Convert each color channel to a vector

# Ensure it's in matrix form (pixels as rows, channels as columns)
img_matrix <- matrix(img_matrix, ncol = 3, byrow = FALSE)
K <- 4  # Number of clusters (e.g., skin vs lesion)

# Initialize values
init_values <- initialize_values(img_matrix, nrow(img_matrix), 3, K)

# Run EM Algorithm
results <- EM_algorithm(img_matrix, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-2)

# Assign each pixel to a cluster based on gamma
segmented <- apply(results$gamma, 1, which.max)  # Assign each pixel to the most probable cluster

# Reshape segmented data back into an image format
segmented_img <- matrix(segmented, nrow = dim(img)[1], ncol = dim(img)[2])

# Display segmented image
imageShow(convert_to_rgb(k, segmented_img))
```



### Crazy

```{r}
# Load Image
img <- readImage("Locura3.jpg")
imageShow(img)

# Convert image into a matrix of pixel values
img_matrix <- apply(img, 3, as.vector)  # Convert each color channel to a vector

# Ensure it's in matrix form (pixels as rows, channels as columns)
img_matrix <- matrix(img_matrix, ncol = 3, byrow = FALSE)
K <- 2  # Number of clusters (e.g., skin vs lesion)

# Initialize values
init_values <- initialize_values(img_matrix, nrow(img_matrix), 3, K)

# Run EM Algorithm
results <- EM_algorithm(img_matrix, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-4)

# Assign each pixel to a cluster based on gamma
segmented <- apply(results$gamma, 1, which.max)  # Assign each pixel to the most probable cluster

# Reshape segmented data back into an image format
segmented_img <- matrix(segmented, nrow = dim(img)[1], ncol = dim(img)[2])

# Display segmented image
imageShow(convert_to_rgb(k, segmented_img))
```

### Mondrian
```{r}
# Load Image
img <- readImage("Composition_A_Mondrian.jpg")
imageShow(img)

# Convert image into a matrix of pixel values
img_matrix <- apply(img, 3, as.vector)  # Convert each color channel to a vector

# Ensure it's in matrix form (pixels as rows, channels as columns)
img_matrix <- matrix(img_matrix, ncol = 3, byrow = FALSE)
K <- 2  # Number of clusters (e.g., skin vs lesion)

# Initialize values
init_values <- initialize_values(img_matrix, nrow(img_matrix), 3, K)

# Run EM Algorithm
results <- EM_algorithm(img_matrix, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-2)

# Assign each pixel to a cluster based on gamma
segmented <- apply(results$gamma, 1, which.max)  # Assign each pixel to the most probable cluster

# Reshape segmented data back into an image format
segmented_img <- matrix(segmented, nrow = dim(img)[1], ncol = dim(img)[2])

# Display segmented image
imageShow(convert_to_rgb(k, segmented_img))
```

```{r}
# Load Image
img <- readImage("Locura3.jpg")
imageShow(img)

# Convert image into a matrix of pixel values
img_matrix <- apply(img, 3, as.vector)  # Convert each color channel to a vector

# Ensure it's in matrix form (pixels as rows, channels as columns)
img_matrix <- matrix(img_matrix, ncol = 3, byrow = FALSE)
K <- 2  # Number of clusters (e.g., skin vs lesion)

# Initialize values
init_values <- initialize_values(img_matrix, nrow(img_matrix), 3, K)

# Run EM Algorithm
results <- EM_algorithm(img_matrix, init_values$Pis, init_values$Mus, init_values$Sigmas, tol = 10^-4)

# Assign each pixel to a cluster based on gamma
segmented <- apply(results$gamma, 1, which.max)  # Assign each pixel to the most probable cluster

# Reshape segmented data back into an image format
segmented_img <- matrix(segmented, nrow = dim(img)[1], ncol = dim(img)[2])

# Display segmented image
imageShow(segmented_img)
```

